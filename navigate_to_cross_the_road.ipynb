{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "773cc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d065b5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the road crossing environment\n",
    "class RoadCrossingEnv:\n",
    "    def __init__(self, width=5):\n",
    "        self.width = width\n",
    "        self.agent_pos = 0\n",
    "        self.goal_pos = width - 1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.agent_pos = 0\n",
    "        return self.agent_pos\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Actions: 0 = left, 1 = right, 2 = stay\n",
    "        if action == 0 and self.agent_pos > 0:\n",
    "            self.agent_pos -= 1\n",
    "        elif action == 1 and self.agent_pos < self.width - 1:\n",
    "            self.agent_pos += 1\n",
    "        \n",
    "        reward = -1  # step penalty\n",
    "        done = False\n",
    "        \n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10\n",
    "            done = True\n",
    "        \n",
    "        return self.agent_pos, reward, done\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        return [0, 1, 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2235430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((env.width, 3))\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(self.env.get_possible_actions())\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        predict = self.q_table[state, action]\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target += self.gamma * np.max(self.q_table[next_state])\n",
    "        self.q_table[state, action] += self.lr * (target - predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade971a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50: total reward = 7\n",
      "Episode 100: total reward = 7\n",
      "Episode 150: total reward = 7\n",
      "Episode 200: total reward = 7\n",
      "Episode 250: total reward = 7\n",
      "Episode 300: total reward = 6\n",
      "Episode 350: total reward = 7\n",
      "Episode 400: total reward = 7\n",
      "Episode 450: total reward = 7\n",
      "Episode 500: total reward = 5\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "#training the agent\n",
    "def train_agent(episodes=500):\n",
    "    env = RoadCrossingEnv(width=5)\n",
    "    agent = QLearningAgent(env)\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        if (ep + 1) % 50 == 0:\n",
    "            print(f\"Episode {ep + 1}: total reward = {total_reward}\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return agent, env\n",
    "\n",
    "agent, env = train_agent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0927c053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing trained agent...\n",
      "Step 0: Position = 0, Action = right\n",
      "Step 1: Position = 1, Action = right\n",
      "Step 2: Position = 2, Action = right\n",
      "Step 3: Position = 3, Action = right\n",
      "Final position: 4\n",
      "Goal reached: Yes\n"
     ]
    }
   ],
   "source": [
    "#testing the trained agent\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "print(\"Testing trained agent...\")\n",
    "\n",
    "while not done and steps < 20:\n",
    "    action = agent.choose_action(state)\n",
    "    action_name = ['left', 'right', 'stay'][action]\n",
    "    print(f\"Step {steps}: Position = {state}, Action = {action_name}\")\n",
    "    state, reward, done = env.step(action)\n",
    "    steps += 1\n",
    "\n",
    "print(f\"Final position: {state}\")\n",
    "print(f\"Goal reached: {'Yes' if done else 'No'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
